{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from os.path import dirname, join\n",
    "from tqdm import tqdm, trange\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch.utils import data as data_utils\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils import data as data_utils\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import numpy as np\n",
    "from numba import jit\n",
    "_frontend = None  # to be set later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnmnkwii.datasets import FileSourceDataset, FileDataSource\n",
    "from os.path import join, expanduser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import generate_cloned_samples, Speech_Dataset\n",
    "import dv3\n",
    "from dv3 import build_deepvoice_3\n",
    "from dv3.hparams import hparams, hparams_debug_string\n",
    "from dv3.train import train as train_dv3\n",
    "from dv3.train import TextDataSource,MelSpecDataSource,LinearSpecDataSource,\\\n",
    "                        PyTorchDataset,PartialyRandomizedSimilarTimeLengthSampler\n",
    "from dv3.train import collate_fn\n",
    "from dv3.deepvoice3_pytorch import frontend\n",
    "from dv3.train import sequence_mask\n",
    "from dv3.train import save_checkpoint as save_checkpoint_dv3\n",
    "from dv3.train import save_states as save_states_dv3\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import generate_cloned_samples, Speech_Dataset\n",
    "from SpeechEmbedding import Encoder\n",
    "from train_encoder import get_cloned_voices,build_encoder,get_speaker_embeddings\n",
    "from train_encoder import load_checkpoint as load_checkpoint_encoder\n",
    "from train_encoder import save_checkpoint as save_checkpoint_encoder\n",
    "from train_encoder import train as train_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"./../data/vctk-preprocessed/\"\n",
    "speaker_id = None\n",
    "\n",
    "dv3.train._frontend = getattr(frontend, hparams.frontend)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input dataset definitions\n",
    "X = FileSourceDataset(TextDataSource(data_root, speaker_id))\n",
    "Mel = FileSourceDataset(MelSpecDataSource(data_root, speaker_id))\n",
    "Y = FileSourceDataset(LinearSpecDataSource(data_root, speaker_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training whole model\n",
      "Training deep voice 3 model\n",
      "dataloader for dv3 prepared\n",
      "Built dv3!\n",
      "Log event path for dv3: log/run-test2018-11-04_15:25:22.820168\n"
     ]
    }
   ],
   "source": [
    "use_cuda = False\n",
    "import os\n",
    "checkpoint_dv3 = None\n",
    "checkpoint_encoder = None\n",
    "speaker_id = None\n",
    "preset_dv3 =None\n",
    "checkpoint_dir = './checkpoint/'\n",
    "\n",
    "data_root = \"./../data/vctk-preprocessed/\"\n",
    "if data_root is None:\n",
    "    data_root = join(dirname(__file__), \"data\", \"ljspeech\")\n",
    "\n",
    "\n",
    "\n",
    "train_dv3_v = None\n",
    "train_encoder_v = None\n",
    "\n",
    "\n",
    "if not train_dv3_v and not train_encoder_v:\n",
    "    print(\"Training whole model\")\n",
    "    train_dv3_v,train_encoder_v= True,True\n",
    "if train_dv3_v:\n",
    "    print(\"Training deep voice 3 model\")\n",
    "elif train_encoder_v:\n",
    "    print(\"Training encoder model\")\n",
    "else:\n",
    "    assert False, \"must be specified wrong args\"\n",
    "\n",
    "os.makedirs(checkpoint_dir , exist_ok=True)\n",
    "\n",
    "# Input dataset definitions\n",
    "X = FileSourceDataset(TextDataSource(data_root, speaker_id))\n",
    "Mel = FileSourceDataset(MelSpecDataSource(data_root, speaker_id))\n",
    "Y = FileSourceDataset(LinearSpecDataSource(data_root, speaker_id))\n",
    "\n",
    "# Prepare sampler\n",
    "frame_lengths = Mel.file_data_source.frame_lengths\n",
    "sampler = PartialyRandomizedSimilarTimeLengthSampler(frame_lengths, batch_size=hparams.batch_size)\n",
    "\n",
    "# Dataset and Dataloader setup\n",
    "dataset = PyTorchDataset(X, Mel, Y)\n",
    "data_loader_dv3 = data_utils.DataLoader(\n",
    "    dataset, batch_size=hparams.batch_size,\n",
    "    num_workers=hparams.num_workers, sampler=sampler,\n",
    "    collate_fn=collate_fn, pin_memory=hparams.pin_memory)\n",
    "print(\"dataloader for dv3 prepared\")\n",
    "\n",
    "dv3.train._frontend = getattr(frontend, hparams.frontend)\n",
    "model_dv3 = build_deepvoice_3(preset_dv3 , checkpoint_dv3)\n",
    "print(\"Built dv3!\")\n",
    "\n",
    "if use_cuda:\n",
    "    model_dv3 = model_dv3.cuda()\n",
    "\n",
    "optimizer_dv3 = optim.Adam(model_dv3.get_trainable_parameters(),\n",
    "                            lr=hparams.initial_learning_rate,\n",
    "                            betas=(hparams.adam_beta1, hparams.adam_beta2),\n",
    "                            eps=hparams.adam_eps,\n",
    "                            weight_decay=hparams.weight_decay)\n",
    "\n",
    "log_event_path = \"log/run-test\" + str(datetime.now()).replace(\" \", \"_\")\n",
    "print(\"Log event path for dv3: {}\".format(log_event_path))\n",
    "writer_dv3 = SummaryWriter(log_dir=log_event_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloned_voices Loaded!\n",
      "Cloning Texts are produced\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'N_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a329447a2c14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mspeaker_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_speaker_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dv3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Encoder is built!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/machine_learning/Speech Processing/Neural-Voice-Cloning-With-Few-Samples/train_encoder.py\u001b[0m in \u001b[0;36mbuild_encoder\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/machine_learning/Speech Processing/Neural-Voice-Cloning-With-Few-Samples/SpeechEmbedding.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprohead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresidual_conv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'N_samples' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ENCODER\n",
    "all_speakers = get_cloned_voices(model_dv3)\n",
    "print(\"Cloning Texts are produced\")\n",
    "\n",
    "speaker_embed = get_speaker_embeddings(model_dv3)\n",
    "\n",
    "encoder = build_encoder()\n",
    "\n",
    "print(\"Encoder is built!\")\n",
    "\n",
    "speech_data_encoder = Speech_Dataset(all_speakers, speaker_embed)\n",
    "\n",
    "criterion_encoder = nn.L1Loss()\n",
    "\n",
    "optimizer_encoder = torch.optim.SGD(encoder.parameters(),lr=0.0006)\n",
    "\n",
    "lambda1_encoder = lambda epoch: 0.6 if epoch%8000==7999 else 1#???????????\n",
    "scheduler_encoder = torch.optim.lr_scheduler.LambdaLR(optimizer_encoder, lr_lambda=lambda1_encoder)\n",
    "\n",
    "data_loader_encoder = data_utils.DataLoader(speech_data_encoder, batch_size=16, shuffle=True, drop_last=True)\n",
    "# Training The Encoder\n",
    "dataiter_encoder = iter(data_loader_encoder)\n",
    "\n",
    "if use_cuda:\n",
    "    encoder = encoder.cuda()\n",
    "\n",
    "if checkpoint_encoder !=None and os.path.isfile(checkpoint_encoder):\n",
    "    encoder, optimizer_encoder = load_checkpoint_encoder(encoder, optimizer_encoder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "global_epoch = 0\n",
    "use_cuda = torch.cuda.is_available()\n",
    "model_encoder = encoder\n",
    "model_dv3 = model_dv3\n",
    "init_lr_dv3 = 0.002\n",
    "nepochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = {}\n",
    "def save_grad(name):\n",
    "    def hook(grad):\n",
    "        grads[name] = grad\n",
    "    return hook\n",
    "\n",
    "# to remember the embeddings of the speakers\n",
    "model_dv3.embed_speakers.weight.register_hook(save_grad('embeddings'))\n",
    "\n",
    "if use_cuda:\n",
    "    model_dv3 = model_dv3.cuda()\n",
    "    model_encoder = model_encoder.cuda()\n",
    "linear_dim = model_dv3.linear_dim\n",
    "r = hparams.outputs_per_step\n",
    "downsample_step = hparams.downsample_step\n",
    "current_lr = init_lr_dv3\n",
    "\n",
    "binary_criterion_dv3 = nn.BCELoss()\n",
    "\n",
    "global global_step, global_epoch\n",
    "while global_epoch < nepochs:\n",
    "    running_loss = 0.0\n",
    "    for step, (x, input_lengths, mel, y, positions, done, target_lengths,\n",
    "               speaker_ids) \\\n",
    "            in tqdm(enumerate(data_loader_dv3)):\n",
    "\n",
    "\n",
    "        model_dv3.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "\n",
    "        #Declaring Requirements\n",
    "        model_dv3.train()\n",
    "        ismultispeaker = speaker_ids is not None\n",
    "        # Learning rate schedule\n",
    "        if hparams.lr_schedule is not None:\n",
    "            lr_schedule_f = getattr(dv3.lrschedule, hparams.lr_schedule)\n",
    "            current_lr = lr_schedule_f(\n",
    "                init_lr_dv3, global_step, **hparams.lr_schedule_kwargs)\n",
    "            for param_group in optimizer_dv3.param_groups:\n",
    "                param_group['lr'] = current_lr\n",
    "        optimizer_dv3.zero_grad()\n",
    "\n",
    "        # Used for Position encoding\n",
    "        text_positions, frame_positions = positions\n",
    "\n",
    "        # Downsample mel spectrogram\n",
    "        if downsample_step > 1:\n",
    "            mel = mel[:, 0::downsample_step, :].contiguous()\n",
    "\n",
    "        # Lengths\n",
    "        input_lengths = input_lengths.long().numpy()\n",
    "        decoder_lengths = target_lengths.long().numpy() // r // downsample_step\n",
    "\n",
    "        voice_encoder = mel.view(mel.shape[0],1,mel.shape[1],mel.shape[2])\n",
    "        # Feed data\n",
    "        x, mel, y = Variable(x), Variable(mel), Variable(y)\n",
    "        voice_encoder = Variable(voice_encoder)\n",
    "        text_positions = Variable(text_positions)\n",
    "        frame_positions = Variable(frame_positions)\n",
    "        done = Variable(done)\n",
    "        target_lengths = Variable(target_lengths)\n",
    "        speaker_ids = Variable(speaker_ids) if ismultispeaker else None\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "            text_positions = text_positions.cuda()\n",
    "            frame_positions = frame_positions.cuda()\n",
    "            y = y.cuda()\n",
    "            mel = mel.cuda()\n",
    "            voice_encoder = voice_encoder.cuda()\n",
    "            done, target_lengths = done.cuda(), target_lengths.cuda()\n",
    "            speaker_ids = speaker_ids.cuda() if ismultispeaker else None\n",
    "\n",
    "        # Create mask if we use masked loss\n",
    "        if hparams.masked_loss_weight > 0:\n",
    "            # decoder output domain mask\n",
    "            decoder_target_mask = sequence_mask(\n",
    "                target_lengths / (r * downsample_step),\n",
    "                max_len=mel.size(1)).unsqueeze(-1)\n",
    "            if downsample_step > 1:\n",
    "                # spectrogram-domain mask\n",
    "                target_mask = sequence_mask(\n",
    "                    target_lengths, max_len=y.size(1)).unsqueeze(-1)\n",
    "            else:\n",
    "                target_mask = decoder_target_mask\n",
    "            # shift mask\n",
    "            decoder_target_mask = decoder_target_mask[:, r:, :]\n",
    "            target_mask = target_mask[:, r:, :]\n",
    "        else:\n",
    "            decoder_target_mask, target_mask = None, None\n",
    "        print(voice_encoder.shape)\n",
    "        #apply encoder model\n",
    "        encoder_out = model_encoder(voice_encoder)\n",
    "\n",
    "\n",
    "        model_dv3.embed_speakers.weight.data = (encoder_out).data\n",
    "        \n",
    "        print(\"set\")\n",
    "        # Apply dv3 model\n",
    "        mel_outputs, linear_outputs, attn, done_hat = model_dv3(\n",
    "                x, mel, speaker_ids=speaker_ids,\n",
    "                text_positions=text_positions, frame_positions=frame_positions,\n",
    "                input_lengths=input_lengths)\n",
    "        \n",
    "        \n",
    "        print(\"foward dv3 done\")\n",
    "        break\n",
    "        # Losses\n",
    "        w = hparams.binary_divergence_weight\n",
    "\n",
    "        # mel:\n",
    "        mel_l1_loss, mel_binary_div = spec_loss(\n",
    "                mel_outputs[:, :-r, :], mel[:, r:, :], decoder_target_mask)\n",
    "        mel_loss = (1 - w) * mel_l1_loss + w * mel_binary_div\n",
    "\n",
    "        # done:\n",
    "        done_loss = binary_criterion(done_hat, done)\n",
    "\n",
    "        # linear:\n",
    "        n_priority_freq = int(hparams.priority_freq / (fs * 0.5) * linear_dim)\n",
    "        linear_l1_loss, linear_binary_div = spec_loss(\n",
    "                linear_outputs[:, :-r, :], y[:, r:, :], target_mask,\n",
    "                priority_bin=n_priority_freq,\n",
    "                priority_w=hparams.priority_freq_weight)\n",
    "        linear_loss = (1 - w) * linear_l1_loss + w * linear_binary_div\n",
    "\n",
    "        # Combine losses\n",
    "        loss_dv3 = mel_loss + linear_loss + done_loss\n",
    "        loss_dv3 = mel_loss + done_loss\n",
    "        loss_dv3 = linear_loss\n",
    "\n",
    "        # attention\n",
    "        if hparams.use_guided_attention:\n",
    "            soft_mask = guided_attentions(input_lengths, decoder_lengths,\n",
    "                                          attn.size(-2),\n",
    "                                          g=hparams.guided_attention_sigma)\n",
    "            soft_mask = Variable(torch.from_numpy(soft_mask))\n",
    "            soft_mask = soft_mask.cuda() if use_cuda else soft_mask\n",
    "            attn_loss = (attn * soft_mask).mean()\n",
    "            loss_dv3 += attn_loss\n",
    "\n",
    "        if global_step > 0 and global_step % checkpoint_interval == 0:\n",
    "            save_states_dv3(\n",
    "                global_step, writer, mel_outputs, linear_outputs, attn,\n",
    "                mel, y, input_lengths, checkpoint_dir)\n",
    "            save_checkpoint_dv3(\n",
    "                model, optimizer, global_step, checkpoint_dir, global_epoch,\n",
    "                train_seq2seq, train_postnet)\n",
    "\n",
    "        if global_step > 0 and global_step % hparams.eval_interval == 0:\n",
    "            eval_model(global_step, writer, model, checkpoint_dir, ismultispeaker)\n",
    "\n",
    "        # Update\n",
    "        loss_dv3.backward()\n",
    "        encoder_out.backward(grads['embeddings'])\n",
    "\n",
    "        optimizer_dv3.step()\n",
    "        optimizer_encoder.step()\n",
    "\n",
    "        # if clip_thresh> 0:\n",
    "        #     grad_norm = torch.nn.utils.clip_grad_norm(\n",
    "        #         model.get_trainable_parameters(), clip_thresh)\n",
    "        global_step += 1\n",
    "        running_loss += loss.data[0]\n",
    "\n",
    "    averaged_loss = running_loss / (len(data_loader))\n",
    "\n",
    "    print(\"Loss: {}\".format(running_loss / (len(data_loader))))\n",
    "\n",
    "    global_epoch += 1\n",
    "\n",
    "\n",
    "# dv3 loss function\n",
    "# backward on that\n",
    "mel_outputs.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "torch.Size([16, 61])\n",
      "torch.Size([16, 196, 80])\n",
      "torch.Size([16, 196, 513])\n",
      "torch.Size([16, 1, 196, 80])\n",
      "----------\n",
      "1\n",
      "1\n",
      "torch.Size([16, 57])\n",
      "torch.Size([16, 180, 80])\n",
      "torch.Size([16, 180, 513])\n",
      "torch.Size([16, 1, 180, 80])\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:03,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "torch.Size([16, 65])\n",
      "torch.Size([16, 136, 80])\n",
      "torch.Size([16, 136, 513])\n",
      "torch.Size([16, 1, 136, 80])\n",
      "----------\n",
      "3\n",
      "3\n",
      "torch.Size([16, 41])\n",
      "torch.Size([16, 132, 80])\n",
      "torch.Size([16, 132, 513])\n",
      "torch.Size([16, 1, 132, 80])\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mehul/machine_learning/Speech Processing/Neural-Voice-Cloning-with-Few-Samples-personal/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/mehul/machine_learning/Speech Processing/Neural-Voice-Cloning-with-Few-Samples-personal/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/mehul/machine_learning/Speech Processing/Neural-Voice-Cloning-With-Few-Samples/dv3/train.py\", line 243, in __getitem__\n",
      "    return text, self.Mel[idx], self.Y[idx], speaker_id\n",
      "Process Process-1:\n",
      "  File \"/home/mehul/machine_learning/Speech Processing/Neural-Voice-Cloning-with-Few-Samples-personal/venv/lib/python3.6/site-packages/nnmnkwii/datasets/__init__.py\", line 146, in __getitem__\n",
      "    return self.__collect_features(paths)\n",
      "  File \"/home/mehul/machine_learning/Speech Processing/Neural-Voice-Cloning-with-Few-Samples-personal/venv/lib/python3.6/site-packages/nnmnkwii/datasets/__init__.py\", line 131, in __collect_features\n",
      "    return self.file_data_source.collect_features(*paths)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mehul/machine_learning/Speech Processing/Neural-Voice-Cloning-With-Few-Samples/dv3/train.py\", line 172, in collect_features\n",
      "    return np.load(path)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mehul/machine_learning/Speech Processing/Neural-Voice-Cloning-with-Few-Samples-personal/venv/lib/python3.6/site-packages/numpy/lib/npyio.py\", line 421, in load\n",
      "    pickle_kwargs=pickle_kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mehul/machine_learning/Speech Processing/Neural-Voice-Cloning-with-Few-Samples-personal/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/mehul/machine_learning/Speech Processing/Neural-Voice-Cloning-with-Few-Samples-personal/venv/lib/python3.6/site-packages/numpy/lib/format.py\", line 661, in read_array\n",
      "    array = numpy.fromfile(fp, dtype=dtype, count=count)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/mehul/machine_learning/Speech Processing/Neural-Voice-Cloning-with-Few-Samples-personal/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/mehul/machine_learning/Speech Processing/Neural-Voice-Cloning-With-Few-Samples/dv3/train.py\", line 243, in __getitem__\n",
      "    return text, self.Mel[idx], self.Y[idx], speaker_id\n",
      "  File \"/home/mehul/machine_learning/Speech Processing/Neural-Voice-Cloning-with-Few-Samples-personal/venv/lib/python3.6/site-packages/nnmnkwii/datasets/__init__.py\", line 146, in __getitem__\n",
      "    return self.__collect_features(paths)\n",
      "  File \"/home/mehul/machine_learning/Speech Processing/Neural-Voice-Cloning-with-Few-Samples-personal/venv/lib/python3.6/site-packages/nnmnkwii/datasets/__init__.py\", line 131, in __collect_features\n",
      "    return self.file_data_source.collect_features(*paths)\n",
      "  File \"/home/mehul/machine_learning/Speech Processing/Neural-Voice-Cloning-With-Few-Samples/dv3/train.py\", line 172, in collect_features\n",
      "    return np.load(path)\n",
      "  File \"/home/mehul/machine_learning/Speech Processing/Neural-Voice-Cloning-with-Few-Samples-personal/venv/lib/python3.6/site-packages/numpy/lib/npyio.py\", line 372, in load\n",
      "    fid = open(file, \"rb\")\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/mehul/machine_learning/Speech Processing/Neural-Voice-Cloning-with-Few-Samples-personal/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-9-d742eafd6b9e>\", line 5, in <module>\n",
      "    speaker_ids) in tqdm(enumerate(data_loader_dv3)):\n",
      "  File \"/home/mehul/machine_learning/Speech Processing/Neural-Voice-Cloning-with-Few-Samples-personal/venv/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 937, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/home/mehul/machine_learning/Speech Processing/Neural-Voice-Cloning-with-Few-Samples-personal/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 330, in __next__\n",
      "    idx, batch = self._get_batch()\n",
      "  File \"/home/mehul/machine_learning/Speech Processing/Neural-Voice-Cloning-with-Few-Samples-personal/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 309, in _get_batch\n",
      "    return self.data_queue.get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mehul/machine_learning/Speech Processing/Neural-Voice-Cloning-with-Few-Samples-personal/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mehul/machine_learning/Speech Processing/Neural-Voice-Cloning-with-Few-Samples-personal/venv/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/mehul/machine_learning/Speech Processing/Neural-Voice-Cloning-with-Few-Samples-personal/venv/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/mehul/machine_learning/Speech Processing/Neural-Voice-Cloning-with-Few-Samples-personal/venv/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.6/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/mehul/machine_learning/Speech Processing/Neural-Voice-Cloning-with-Few-Samples-personal/venv/lib/python3.6/posixpath.py\", line 388, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/mehul/machine_learning/Speech Processing/Neural-Voice-Cloning-with-Few-Samples-personal/venv/lib/python3.6/posixpath.py\", line 422, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/mehul/machine_learning/Speech Processing/Neural-Voice-Cloning-with-Few-Samples-personal/venv/lib/python3.6/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "  File \"/home/mehul/machine_learning/Speech Processing/Neural-Voice-Cloning-with-Few-Samples-personal/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 227, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 8586) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# linear_dim = model_dv3.linear_dim\n",
    "r = hparams.outputs_per_step\n",
    "downsample_step = hparams.downsample_step\n",
    "for step, (x, input_lengths, mel, y, positions, done, target_lengths,\n",
    "                   speaker_ids) in tqdm(enumerate(data_loader_dv3)):\n",
    "    print(step)\n",
    "    ismultispeaker = speaker_ids is not None\n",
    "    text_positions, frame_positions = positions\n",
    "    input_lengths = input_lengths.long().numpy()\n",
    "    decoder_lengths = target_lengths.long().numpy() // r // downsample_step\n",
    "    # Feed data\n",
    "    voice_encoder = mel.view(mel.shape[0],1,mel.shape[1],mel.shape[2])\n",
    "    x, mel, y = Variable(x), Variable(mel), Variable(y)\n",
    "    voice_encoder = mel.view(mel.shape[0],1,mel.shape[1],mel.shape[2])\n",
    "    text_positions = Variable(text_positions)\n",
    "    frame_positions = Variable(frame_positions)\n",
    "    done = Variable(done)\n",
    "    target_lengths = Variable(target_lengths)\n",
    "    speaker_ids = Variable(speaker_ids) if ismultispeaker else None\n",
    "    \n",
    "    print(step)\n",
    "    print(x.shape)\n",
    "    print(mel.shape)\n",
    "    print(y.shape)\n",
    "    print(voice_encoder.shape)\n",
    "    print('-'*10)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloned_voices Loaded!\n",
      "Cloning Texts are produced\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108, 23, 201, 80)\n"
     ]
    }
   ],
   "source": [
    "from utils import Speech_Dataset\n",
    "speech_data = Speech_Dataset(all_speakers, speaker_embed)\n",
    "data_loader = data_utils.DataLoader(speech_data, batch_size=16, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 23, 201, 80])\n",
      "torch.Size([16, 23, 201, 80])\n",
      "torch.Size([16, 23, 201, 80])\n",
      "torch.Size([16, 23, 201, 80])\n",
      "torch.Size([16, 23, 201, 80])\n",
      "torch.Size([16, 23, 201, 80])\n"
     ]
    }
   ],
   "source": [
    "for i_element, element in enumerate(data_loader):\n",
    "    voice, embed = element[0], element[1]\n",
    "    print(voice.shape)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 23, 123, 80])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(16,23 , 123 , 80)\n",
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mehul/machine_learning/Speech Processing/Neural-Voice-Cloning-with-Few-Samples-personal/venv/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 23])\n",
      "torch.Size([16, 23])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0f2a65419a44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/machine_learning/Speech Processing/Neural-Voice-Cloning-with-Few-Samples-personal/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/machine_learning/Speech Processing/Neural-Voice-Cloning-With-Few-Samples/SpeechEmbedding.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# x = torch.unsqueeze(x, dim=2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "encoder(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
